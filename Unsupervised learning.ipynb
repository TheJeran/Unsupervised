{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.        \n",
    "                 q_ij = 1/(1+dist(x_i, Âµ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "def encoders(input_shape=(28,28,1),n_classes=10):\n",
    "    enc_input = Input(input_shape,name='input_layer')\n",
    "    enc = Conv2D(16,3,padding='same',data_format='channels_last',activation='relu',name='enc_1')(enc_input)\n",
    "    enc = Conv2D(32,3,padding='same',data_format='channels_last',activation='relu',name='enc_2')(enc)\n",
    "    enc = MaxPool2D(padding='same')(enc)\n",
    "    enc = Conv2D(32,3,padding='same',data_format='channels_last',activation='relu',name='enc_3')(enc)\n",
    "    shape = K.int_shape(enc)\n",
    "    enc = Flatten()(enc)\n",
    "    enc_output = Dense(n_classes,activation='relu')(enc)\n",
    "    \n",
    "    dec_input = Dense(np.prod(shape[1:]),activation='relu',name='dec_input')(enc_output)\n",
    "    dec = Reshape(shape[1:],name='dec_1')(dec_input)\n",
    "    dec = UpSampling2D(data_format='channels_last',name='dec_2')(dec)\n",
    "    dec = Conv2D(32,3,padding='same',data_format='channels_last',activation='relu',name='dec_3')(dec)\n",
    "    dec = Conv2D(16,3,padding='same',data_format='channels_last',activation='relu',name='dec_4')(dec)\n",
    "    dec_output = Conv2D(1,3,padding='same',data_format='channels_last',activation='sigmoid',name='dec_output')(dec) \n",
    "    \n",
    "    return Model(enc_input,dec_output,name='AE'), Model(enc_input,enc_output,name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE, encoder = encoders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above line we created both the autoencoder to extract features from the images as well as well as the encoder ending at our 10 unit dense layer to append the clustering layer later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "  1/469 [..............................] - ETA: 0s - loss: 0.2316WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.0210s). Check your callbacks.\n",
      "469/469 [==============================] - 16s 33ms/step - loss: 0.0459\n",
      "Epoch 2/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0273\n",
      "Epoch 3/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0240\n",
      "Epoch 4/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0215\n",
      "Epoch 5/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0206\n",
      "Epoch 6/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0200\n",
      "Epoch 7/64\n",
      "469/469 [==============================] - 16s 33ms/step - loss: 0.0195\n",
      "Epoch 8/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0192\n",
      "Epoch 9/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0189\n",
      "Epoch 10/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0186\n",
      "Epoch 11/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0184\n",
      "Epoch 12/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0182\n",
      "Epoch 13/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0180\n",
      "Epoch 14/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0179\n",
      "Epoch 15/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0178\n",
      "Epoch 16/64\n",
      "469/469 [==============================] - 16s 33ms/step - loss: 0.0177\n",
      "Epoch 17/64\n",
      "469/469 [==============================] - 16s 33ms/step - loss: 0.0176\n",
      "Epoch 18/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0175\n",
      "Epoch 19/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0174\n",
      "Epoch 20/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0173\n",
      "Epoch 21/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0172\n",
      "Epoch 22/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0171\n",
      "Epoch 23/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0171\n",
      "Epoch 24/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0170\n",
      "Epoch 25/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0170\n",
      "Epoch 26/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0169\n",
      "Epoch 27/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0168\n",
      "Epoch 28/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0168\n",
      "Epoch 29/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0168\n",
      "Epoch 30/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0167\n",
      "Epoch 31/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0166\n",
      "Epoch 32/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0166\n",
      "Epoch 33/64\n",
      "469/469 [==============================] - 16s 33ms/step - loss: 0.0166\n",
      "Epoch 34/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0166\n",
      "Epoch 35/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0165\n",
      "Epoch 36/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0165\n",
      "Epoch 37/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0164\n",
      "Epoch 38/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0164\n",
      "Epoch 39/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0164\n",
      "Epoch 40/64\n",
      "469/469 [==============================] - 16s 33ms/step - loss: 0.0163\n",
      "Epoch 41/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0163\n",
      "Epoch 42/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0163\n",
      "Epoch 43/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0163\n",
      "Epoch 44/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0162\n",
      "Epoch 45/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0162\n",
      "Epoch 46/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0162\n",
      "Epoch 47/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0162\n",
      "Epoch 48/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0161\n",
      "Epoch 49/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0161\n",
      "Epoch 50/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0161\n",
      "Epoch 51/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0161\n",
      "Epoch 52/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0161\n",
      "Epoch 53/64\n",
      "469/469 [==============================] - 16s 33ms/step - loss: 0.0160\n",
      "Epoch 54/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0160\n",
      "Epoch 55/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0160\n",
      "Epoch 56/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0160\n",
      "Epoch 57/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0160\n",
      "Epoch 58/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0159\n",
      "Epoch 59/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0159\n",
      "Epoch 60/64\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0159\n",
      "Epoch 61/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0159\n",
      "Epoch 62/64\n",
      "469/469 [==============================] - 16s 33ms/step - loss: 0.0159\n",
      "Epoch 63/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0158\n",
      "Epoch 64/64\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e75a5e2280>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.compile(optimizer='adam',loss='mse')\n",
    "AE.fit(x_train,x_train,batch_size=128,epochs=64)\n",
    "AE.save_weights('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x21a9e69aaf0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.compile(optimizer='adam',loss='mse')\n",
    "AE.load_weights('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "cluster_layer = ClusteringLayer(n_clusters,name='clustering')(encoder.output)\n",
    "\n",
    "clusterer = Model(inputs=encoder.input,outputs=cluster_layer)\n",
    "### This is the cluster model. The cluster layer is still a bit of a blackbox at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "enc_1 (Conv2D)               (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "enc_2 (Conv2D)               (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "enc_3 (Conv2D)               (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                62730     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 10)                100       \n",
      "=================================================================\n",
      "Total params: 76,878\n",
      "Trainable params: 76,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clusterer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters,n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7146772228320138"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "normalized_mutual_info_score(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above metric is the normalized mutual info score. It is assuming two sets are equivalent but with different values for classes. So a set of [0,0,1,1] and [1,1,0,0] has a score of 1.0 because it assumes both sets are describing the same thing just with different values for each class. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
